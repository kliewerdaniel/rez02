---
layout: post
title:  Basic RAG
date:   2024-12-01 07:42:44 -0500
---
# Building a Robust Retrieval-Augmented Generation System with LangChain and OpenAI

**Table of Contents**

- [Introduction](#introduction)
- [Prerequisites](#prerequisites)
- [Setting Up the Environment](#setting-up-the-environment)
- [Understanding the Code](#understanding-the-code)
  - [1. Loading Environment Variables](#1-loading-environment-variables)
  - [2. Importing Necessary Libraries](#2-importing-necessary-libraries)
  - [3. Loading and Splitting Documents](#3-loading-and-splitting-documents)
  - [4. Creating Embeddings and Vector Store](#4-creating-embeddings-and-vector-store)
  - [5. Setting Up Retrieval and LLM Chain](#5-setting-up-retrieval-and-llm-chain)
  - [6. Interactive Querying](#6-interactive-querying)
- [Implementing for More Robust Systems](#implementing-for-more-robust-systems)
  - [1. Enhanced Error Handling and Logging](#1-enhanced-error-handling-and-logging)
  - [2. Supporting Additional File Types](#2-supporting-additional-file-types)
  - [3. Optimizing Text Splitting Strategy](#3-optimizing-text-splitting-strategy)
  - [4. Advanced Retrieval Techniques](#4-advanced-retrieval-techniques)
  - [5. Implementing Caching Mechanisms](#5-implementing-caching-mechanisms)
  - [6. Scaling with Cloud-Based Vector Stores](#6-scaling-with-cloud-based-vector-stores)
  - [7. Security Best Practices](#7-security-best-practices)
- [Conclusion](#conclusion)
- [References](#references)

---

## Introduction

In the realm of artificial intelligence, **Retrieval-Augmented Generation (RAG)** has emerged as a powerful technique to enhance the capabilities of language models. By combining retrieval mechanisms with generative models, RAG systems can access external knowledge bases, leading to more accurate and contextually relevant responses.

This blog post will guide you through implementing a RAG system using the following technologies:

- **[LangChain](https://github.com/hwchase17/langchain)**: A framework for developing applications powered by language models.
- **[OpenAI](https://openai.com/)**: Provides access to powerful language models like GPT-3 and GPT-4.
- **[ChromaDB](https://www.trychroma.com/)**: A vector database for efficient storage and retrieval of embeddings.
- **Additional Libraries**: Including `pinecone-client`, `tiktoken`, `sentence-transformers`, `python-dotenv`, `PyPDF2`, `langchain-community`, `langchain-openai`, and `langchain-chroma`.

We'll walk through a Python script that processes documents from a folder, creates embeddings, stores them in a vector database, and sets up an interactive question-answering system.

---

## Prerequisites

Before we begin, ensure you have the following:

- **Python 3.7 or higher** installed on your machine.
- An **OpenAI API key**. You can obtain one by signing up on the [OpenAI website](https://platform.openai.com/).
- Familiarity with Python programming and virtual environments.
- Basic understanding of embeddings and vector databases.

---

## Setting Up the Environment

First, let's set up a virtual environment and install the required libraries.

```bash
# Create and activate a virtual environment
python3 -m venv rag-env
source rag-env/bin/activate  # For Windows, use 'rag-env\Scripts\activate'

# Upgrade pip
pip install --upgrade pip

# Install required packages
pip install langchain openai chromadb pinecone-client tiktoken
pip install sentence-transformers python-dotenv PyPDF2
pip install langchain-community langchain-openai langchain-chroma
```

---

## Understanding the Code

Below is the Python script we'll be discussing:

```python
import os
import sys
import glob
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Updated imports
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma.vectorstores import Chroma
from langchain_openai.llms import OpenAI
from langchain.chains import RetrievalQA

# Updated document loaders
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

def main():
   # Load OpenAI API key
   openai_api_key = os.getenv("OPENAI_API_KEY")
   if not openai_api_key:
       print("Please set your OPENAI_API_KEY in the .env file.")
       sys.exit(1)
  
   # Define the folder path (change 'data' to your folder name)
   folder_path = './data'
   if not os.path.exists(folder_path):
       print(f"Folder '{folder_path}' does not exist.")
       sys.exit(1)
  
   # Read all files in the folder
   documents = []
   for filepath in glob.glob(os.path.join(folder_path, '**/*.*'), recursive=True):
       if os.path.isfile(filepath):
           ext = os.path.splitext(filepath)[1].lower()
           try:
               if ext == '.txt':
                   loader = TextLoader(filepath, encoding='utf-8')
                   documents.extend(loader.load_and_split())
               elif ext == '.pdf':
                   loader = PyPDFLoader(filepath)
                   documents.extend(loader.load_and_split())
               else:
                   print(f"Unsupported file format: {filepath}")
           except Exception as e:
               print(f"Error reading '{filepath}': {e}")
  
   if not documents:
       print("No documents found in the folder.")
       sys.exit(1)
  
   # Split documents into chunks
   text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
   texts = text_splitter.split_documents(documents)
  
   # Initialize embeddings and vector store
   embeddings = OpenAIEmbeddings()
   vector_store = Chroma(embedding_function=embeddings, persist_directory="./chroma_store")
  
   # Add texts to vector store in batches
   batch_size = 500  # Adjust this number as needed
   for i in range(0, len(texts), batch_size):
       batch_texts = texts[i:i+batch_size]
       vector_store.add_documents(batch_texts)
  
   # Set up retriever
   retriever = vector_store.as_retriever(search_kwargs={"k": 3})
  
   # Set up the language model
   llm = OpenAI(temperature=0.7)
  
   # Create the RetrievalQA chain
   qa_chain = RetrievalQA.from_chain_type(
       llm=llm,
       chain_type="stuff",  # Options: 'stuff', 'map_reduce', 'refine', 'map_rerank'
       retriever=retriever
   )
  
   # Interactive prompt for user queries
   print("The system is ready. You can now ask questions about the content.")
   while True:
       query = input("Enter your question (or type 'exit' to quit): ")
       if query.lower() in ('exit', 'quit'):
           break
       try:
           response = qa_chain.run(query)
           print(f"\nAnswer: {response}\n")
       except Exception as e:
           print(f"An error occurred: {e}\n")
          
if __name__ == "__main__":
   main()
```

Let's break down each part of the code.

### 1. Loading Environment Variables

We use `python-dotenv` to load environment variables from a `.env` file. This is where we'll store our OpenAI API key securely.

```python
import os
import sys
from dotenv import load_dotenv

load_dotenv()

openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    print("Please set your OPENAI_API_KEY in the .env file.")
    sys.exit(1)
```

**Instructions:**

- Create a `.env` file in your project directory.
- Add your OpenAI API key:
  ```
  OPENAI_API_KEY=your_openai_api_key_here
  ```

### 2. Importing Necessary Libraries

We import updated modules from `langchain` and associated packages.

```python
# Embeddings and vector store
from langchain_openai.embeddings import OpenAIEmbeddings
from langchain_chroma.vectorstores import Chroma
from langchain_openai.llms import OpenAI
from langchain.chains import RetrievalQA

# Document loaders and text splitter
from langchain_community.document_loaders import TextLoader, PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
```

**Note:** Ensure all packages are up-to-date to avoid deprecation warnings.

### 3. Loading and Splitting Documents

The script reads all `.txt` and `.pdf` files from the specified folder and splits them into manageable chunks.

```python
import glob

folder_path = './data'
if not os.path.exists(folder_path):
    print(f"Folder '{folder_path}' does not exist.")
    sys.exit(1)

documents = []
for filepath in glob.glob(os.path.join(folder_path, '**/*.*'), recursive=True):
    if os.path.isfile(filepath):
        ext = os.path.splitext(filepath)[1].lower()
        try:
            if ext == '.txt':
                loader = TextLoader(filepath, encoding='utf-8')
                documents.extend(loader.load_and_split())
            elif ext == '.pdf':
                loader = PyPDFLoader(filepath)
                documents.extend(loader.load_and_split())
            else:
                print(f"Unsupported file format: {filepath}")
        except Exception as e:
            print(f"Error reading '{filepath}': {e}")

if not documents:
    print("No documents found in the folder.")
    sys.exit(1)

# Split documents into chunks
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
texts = text_splitter.split_documents(documents)
```

**Instructions:**

- Place your `.txt` and `.pdf` files in the `./data` folder.
- Adjust `chunk_size` and `chunk_overlap` as needed.

### 4. Creating Embeddings and Vector Store

We initialize embeddings using OpenAI's models and store them in ChromaDB.

```python
embeddings = OpenAIEmbeddings()
vector_store = Chroma(embedding_function=embeddings, persist_directory="./chroma_store")

batch_size = 500  # Adjust this number as needed
for i in range(0, len(texts), batch_size):
    batch_texts = texts[i:i+batch_size]
    vector_store.add_documents(batch_texts)
```

**Explanation:**

- **Embeddings:** Convert text into numerical vectors that capture semantic meaning.
- **Vector Store:** Stores these embeddings for efficient retrieval.

### 5. Setting Up Retrieval and LLM Chain

We set up the retriever and connect it to the OpenAI language model using LangChain's `RetrievalQA` chain.

```python
retriever = vector_store.as_retriever(search_kwargs={"k": 3})

llm = OpenAI(temperature=0.7)

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # Options: 'stuff', 'map_reduce', 'refine', 'map_rerank'
    retriever=retriever
)
```

**Explanation:**

- **Retriever:** Fetches the most relevant documents based on the query.
- **LLM Chain:** Uses the language model to generate answers based on retrieved documents.

### 6. Interactive Querying

We create an interactive loop where users can input queries and receive answers.

```python
print("The system is ready. You can now ask questions about the content.")
while True:
    query = input("Enter your question (or type 'exit' to quit): ")
    if query.lower() in ('exit', 'quit'):
        break
    try:
        response = qa_chain.run(query)
        print(f"\nAnswer: {response}\n")
    except Exception as e:
        print(f"An error occurred: {e}\n")
```

---

## Implementing for More Robust Systems

To enhance the robustness and scalability of the system, consider the following improvements.

### 1. Enhanced Error Handling and Logging

Implement more comprehensive error handling and logging mechanisms to make debugging easier.

**Example:**

```python
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Replace print statements with logger
logger.info("The system is ready. You can now ask questions about the content.")
```

### 2. Supporting Additional File Types

Extend support to more file formats like `.docx`, `.html`, or `.csv` by using appropriate loaders.

**Example:**

```python
from langchain_community.document_loaders import UnstructuredWordDocumentLoader, UnstructuredHTMLLoader

# Add support in the file processing loop
elif ext == '.docx':
    loader = UnstructuredWordDocumentLoader(filepath)
    documents.extend(loader.load_and_split())
elif ext == '.html':
    loader = UnstructuredHTMLLoader(filepath)
    documents.extend(loader.load_and_split())
```

### 3. Optimizing Text Splitting Strategy

Fine-tune the `chunk_size` and `chunk_overlap` based on the nature of your documents to balance context and performance.

**Example:**

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=300)
```

### 4. Advanced Retrieval Techniques

Enhance the retriever by using metadata filtering or experimenting with different similarity metrics.

**Example:**

```python
retriever = vector_store.as_retriever(
    search_kwargs={"k": 5},
    metadata_filters={"category": "finance"}
)
```

### 5. Implementing Caching Mechanisms

Use caching to reduce API calls to OpenAI and improve response times.

**Example:**

```python
from langchain.cache import InMemoryCache

# Enable caching
qa_chain.cache = InMemoryCache()
```

### 6. Scaling with Cloud-Based Vector Stores

For larger datasets, consider using a cloud-based vector store like Pinecone.

**Example with Pinecone:**

```python
import pinecone

pinecone.init(api_key="your_pinecone_api_key", environment="your_pinecone_environment")

# Create an index
index_name = "your_index_name"
if index_name not in pinecone.list_indexes():
    pinecone.create_index(index_name, dimension=embeddings.dimension)

from langchain_pinecone.vectorstores import Pinecone

index = pinecone.Index(index_name)
vector_store = Pinecone(index, embedding_function=embeddings)
```

### 7. Security Best Practices

Ensure the security of your system:

- **API Key Management:** Use environment variables or secret management tools.
- **Input Sanitization:** Validate and sanitize user inputs to prevent injection attacks.

---

## Conclusion

Building a Retrieval-Augmented Generation system using LangChain and OpenAI empowers you to create intelligent applications capable of understanding and utilizing vast amounts of textual data. By implementing the enhancements discussed, you can develop a more robust, scalable, and efficient system tailored to your specific needs.

**Next Steps:**

- **Experiment:** Try different models and chain types to see what works best for your use case.
- **Scale:** Consider deploying your system using cloud services for better scalability.
- **Stay Updated:** Keep an eye on updates to the libraries and tools used.

---

## References

- [LangChain Documentation](https://python.langchain.com/)
- [OpenAI API](https://platform.openai.com/docs/api-reference/introduction)
- [ChromaDB](https://www.trychroma.com/)
- [Pinecone](https://www.pinecone.io/)
- [LangChain Community GitHub](https://github.com/hwchase17/langchain)

---
